{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA dostępna: True\n",
      "Nazwa GPU: NVIDIA GeForce RTX 3070 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchaudio.transforms as T\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "print(\"CUDA dostępna:\", torch.cuda.is_available())\n",
    "print(\"Nazwa GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Brak\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG]: Liczba próbek treningowych: 7398\n",
      "[LOG]: Liczba próbek walidacyjnych: 1055\n",
      "[LOG]: Liczba próbek testowych: 1033\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "NUM_SAMPLES = 16000\n",
    "N_MELS = 128\n",
    "N_FFT = 1024\n",
    "HOP_LENGTH = 512\n",
    "\n",
    "class SpeechCommandsDataset(Dataset):\n",
    "    def __init__(self, root_dir, classes, split='train', transform=None):\n",
    "        self.classes = classes\n",
    "        self.transform = transform\n",
    "        self.resampler = T.Resample(orig_freq=16000, new_freq=SAMPLE_RATE)\n",
    "\n",
    "        val_list = self._load_list(os.path.join(root_dir, '..', 'validation_list.txt'))\n",
    "        test_list = self._load_list(os.path.join(root_dir, '..', 'testing_list.txt'))\n",
    "\n",
    "        include = set()\n",
    "        exclude = set()\n",
    "\n",
    "        if split == 'train':\n",
    "            exclude = set(val_list + test_list)\n",
    "        elif split == 'val':\n",
    "            include = set(val_list)\n",
    "        elif split == 'test':\n",
    "            include = set(test_list)\n",
    "        else:\n",
    "            raise ValueError(\"split must be one of: 'train', 'val', 'test'\")\n",
    "\n",
    "        self.filepaths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for label, cls in enumerate(classes):\n",
    "            cls_dir = os.path.join(root_dir, cls)\n",
    "            if not os.path.isdir(cls_dir):\n",
    "                continue\n",
    "            for file in os.listdir(cls_dir):\n",
    "                if not file.endswith('.wav'):\n",
    "                    continue\n",
    "                rel_path = os.path.join(cls, file)\n",
    "                if (split == 'train' and rel_path in exclude) or (split in ['val', 'test'] and rel_path not in include):\n",
    "                    continue\n",
    "                self.filepaths.append(os.path.join(cls_dir, file))\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def _load_list(self, file_path):\n",
    "        if not os.path.exists(file_path):\n",
    "            return []\n",
    "        with open(file_path, 'r') as f:\n",
    "            return [line.strip() for line in f.readlines()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sample_rate = torchaudio.load(self.filepaths[idx])\n",
    "        waveform = self.resampler(waveform)\n",
    "\n",
    "        if waveform.shape[1] < NUM_SAMPLES:\n",
    "            padding = (0, NUM_SAMPLES - waveform.shape[1])\n",
    "            waveform = torch.nn.functional.pad(waveform, padding)\n",
    "        else:\n",
    "            waveform = waveform[:, :NUM_SAMPLES]\n",
    "\n",
    "        if self.transform:\n",
    "            spec = self.transform(waveform)\n",
    "            spec = spec.squeeze(0)\n",
    "        else:\n",
    "            spec = waveform.squeeze(0)\n",
    "\n",
    "        return spec, self.labels[idx]\n",
    "\n",
    "mel_spectrogram = torch.nn.Sequential(\n",
    "    T.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_fft=N_FFT,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        n_mels=N_MELS\n",
    "    ),\n",
    "    T.AmplitudeToDB()\n",
    ")\n",
    "\n",
    "classes = ['yes', 'no', 'up', 'down']\n",
    "\n",
    "root_audio = \"./data/train/audio/\"\n",
    "\n",
    "train_dataset = SpeechCommandsDataset(root_dir=root_audio, classes=classes, split='train', transform=mel_spectrogram)\n",
    "val_dataset = SpeechCommandsDataset(root_dir=root_audio, classes=classes, split='val', transform=mel_spectrogram)\n",
    "test_dataset = SpeechCommandsDataset(root_dir=root_audio, classes=classes, split='test', transform=mel_spectrogram)\n",
    "\n",
    "print(f\"[LOG]: Liczba próbek treningowych: {len(train_dataset)}\")\n",
    "print(f\"[LOG]: Liczba próbek walidacyjnych: {len(val_dataset)}\")\n",
    "print(f\"[LOG]: Liczba próbek testowych: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Positional Encoding Module\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "# Single Encoder Layer\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Multi-head attention with residual and norm\n",
    "        attn_output, _ = self.self_attn(src, src, src)\n",
    "        src = src + self.dropout1(attn_output)\n",
    "        src = self.norm1(src)\n",
    "\n",
    "        # Feedforward network with residual and norm\n",
    "        ff_output = self.linear2(self.dropout(F.relu(self.linear1(src))))\n",
    "        src = src + self.dropout2(ff_output)\n",
    "        src = self.norm2(src)\n",
    "        return src\n",
    "\n",
    "# Full Transformer Encoder\n",
    "class AudioTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=128, nhead=4, num_layers=3, num_classes=2, dim_feedforward=256, dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, mel, time] -> [batch, time, mel]\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.input_proj(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        x = x.mean(dim=1)  # global average pooling\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, epoch, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    batch_losses = []\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val = loss.item()\n",
    "        total_loss += loss_val\n",
    "        batch_losses.append(loss_val)\n",
    "\n",
    "        pred = output.argmax(dim=1)\n",
    "        correct += pred.eq(target).sum().item()\n",
    "\n",
    "    acc = 100. * correct / len(loader.dataset)\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"Train Epoch: {epoch}\\tLoss: {avg_loss:.4f}\\tAccuracy: {acc:.2f}%\")\n",
    "    return avg_loss, acc, batch_losses\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    batch_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            loss_val = loss.item()\n",
    "            total_loss += loss_val\n",
    "            batch_losses.append(loss_val)\n",
    "\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "\n",
    "    acc = 100. * correct / len(loader.dataset)\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"Validation: Average loss: {avg_loss:.4f}\\tAccuracy: {acc:.2f}%\")\n",
    "    return avg_loss, acc, batch_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report, confusion_matrix, roc_curve\n",
    "import seaborn as sns\n",
    "\n",
    "def score_model(y_true, y_pred, model_name = \"Model\"):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d')\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Metrics for {model_name}:\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.2f}\")\n",
    "    print(f\"Classification Report:\\n{classification_report(y_true, y_pred)}\")\n",
    "\n",
    "\n",
    "def training_details_plot(batch_loses_train, batch_loses_validate, title, smoothness = 0.9):\n",
    "    def smooth_curve(data, weight):\n",
    "        smoothed = []\n",
    "        last = data[0]\n",
    "        for point in data:\n",
    "            smoothed_val = last * weight + (1 - weight) * point\n",
    "            smoothed.append(smoothed_val)\n",
    "            last = smoothed_val\n",
    "        return smoothed\n",
    "    train_loses = smooth_curve(batch_loses_train, smoothness)\n",
    "    valid_loses = smooth_curve(batch_loses_validate, smoothness)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(train_loses, label=\"Train Loss\")\n",
    "    plt.plot(valid_loses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "results = defaultdict(dict)\n",
    "model_save_dir = \"saved_transformer_models\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "enc_layers = [1, 3]\n",
    "heads = [2, 4]\n",
    "dim_FF = [128, 256]\n",
    "\n",
    "for el in enc_layers:\n",
    "    for h in heads:\n",
    "        for d in dim_FF:\n",
    "\n",
    "            config_key = f\"layers{el}_heads{h}_ff{d}\"\n",
    "            print(f\"[LOG] === Start training model: {config_key} ===\")\n",
    "\n",
    "            model = AudioTransformer(\n",
    "                input_dim = N_MELS,\n",
    "                d_model = 128,\n",
    "                nhead = h,\n",
    "                num_layers = el,\n",
    "                num_classes = len(classes),\n",
    "                dim_feedforward = d\n",
    "            ).to(device)\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            best_val_acc = 0\n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "            val_accuracies = []\n",
    "            batch_losses_train_all = []\n",
    "            batch_losses_valid_all = []\n",
    "\n",
    "            model_path = os.path.join(model_save_dir, f\"model_{config_key}.pth\")\n",
    "\n",
    "            for epoch in range(1, 3):\n",
    "                train_loss, train_acc, batch_train = train(model, train_loader, optimizer, criterion, epoch, device)\n",
    "                val_loss, val_acc, batch_val = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                val_accuracies.append(val_acc)\n",
    "                batch_losses_train_all.extend(batch_train)\n",
    "                batch_losses_valid_all.extend(batch_val)\n",
    "            \n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"[LOG] Best model saved: {model_path} (Val Acc: {val_acc:.2f}%)\")\n",
    "            \n",
    "            results[config_key] = {\n",
    "                \"train_losses\": train_losses,\n",
    "                \"val_losses\": val_losses,\n",
    "                \"val_accuracies\": val_accuracies,\n",
    "                \"batch_losses_train\": batch_losses_train_all,\n",
    "                \"batch_losses_valid\": batch_losses_valid_all,\n",
    "                \"model_path\": model_path,\n",
    "                \"best_val_acc\": best_val_acc,\n",
    "                \"params\": {\n",
    "                    \"layers\": el,\n",
    "                    \"heads\": h,\n",
    "                    \"ff_dim\": d\n",
    "                }\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config_name, data in results.items():\n",
    "        print(f\"\\n=== Configuration: {config_name} ===\")\n",
    "\n",
    "        # Odtwórz model\n",
    "        model = AudioTransformer(\n",
    "            input_dim=N_MELS,\n",
    "            d_model=128,\n",
    "            nhead=data[\"params\"][\"heads\"],\n",
    "            num_layers=data[\"params\"][\"layers\"],\n",
    "            num_classes=len(classes),\n",
    "            dim_feedforward=data[\"params\"][\"ff_dim\"]\n",
    "        ).to(device)\n",
    "\n",
    "        model.load_state_dict(torch.load(data[\"model_path\"]))\n",
    "        model.eval()\n",
    "\n",
    "        # Rysuj loss plot\n",
    "        training_details_plot(\n",
    "            batch_loses_train=data[\"batch_losses_train\"],\n",
    "            batch_loses_validate=data[\"batch_losses_valid\"],\n",
    "            title=f\"Loss per iteration - {config_name}\",\n",
    "            smoothness=0.98\n",
    "        )\n",
    "\n",
    "        # Predykcja na zbiorze testowym\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Wyświetl metryki i confusion matrix\n",
    "        score_model(all_labels, all_preds, model_name=config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechCommandRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, N_MELS, TIME] → chcemy [B, TIME, N_MELS]\n",
    "        x = x.transpose(1, 2)\n",
    "        output, (hidden, _) = self.lstm(x)  # hidden: [num_layers, B, hidden_size]\n",
    "        out = hidden[-1]  # bierzemy wyjście z ostatniej warstwy RNN\n",
    "        return self.classifier(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Stwórz katalog na modele\n",
    "model_save_dir_rnn = \"saved_rnn_models\"\n",
    "os.makedirs(model_save_dir_rnn, exist_ok=True)\n",
    "\n",
    "# Wyniki\n",
    "rnn_results = defaultdict(dict)\n",
    "\n",
    "# Hiperparametry do testów\n",
    "hidden_sizes = [64, 128]\n",
    "num_layers_list = [2, 3]\n",
    "\n",
    "for hidden_size in hidden_sizes:\n",
    "    for num_layers in num_layers_list:\n",
    "\n",
    "        config_key = f\"hidden{hidden_size}_layers{num_layers}\"\n",
    "        print(f\"[LOG] === Start training RNN model: {config_key} ===\")\n",
    "\n",
    "        rnn_model = SpeechCommandRNN(\n",
    "            input_size=N_MELS,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            num_classes=len(classes)\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer_rnn = torch.optim.Adam(rnn_model.parameters(), lr=1e-4)\n",
    "        criterion_rnn = nn.CrossEntropyLoss()\n",
    "\n",
    "        best_val_acc = 0\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        batch_losses_train_all = []\n",
    "        batch_losses_valid_all = []\n",
    "\n",
    "        model_path = os.path.join(model_save_dir_rnn, f\"rnn_model_{config_key}.pth\")\n",
    "\n",
    "        for epoch in range(1, 11):  # możesz dać np. 10 epok\n",
    "            train_loss, train_acc, batch_train = train(rnn_model, train_loader, optimizer_rnn, criterion_rnn, epoch, device)\n",
    "            val_loss, val_acc, batch_val = evaluate(rnn_model, val_loader, criterion_rnn, device)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_acc)\n",
    "            batch_losses_train_all.extend(batch_train)\n",
    "            batch_losses_valid_all.extend(batch_val)\n",
    "\n",
    "\n",
    "        torch.save(rnn_model.state_dict(), model_path)\n",
    "        print(f\"[LOG] New best RNN model saved: {model_path} (Val Acc: {val_acc:.2f}%)\")\n",
    "\n",
    "        rnn_results[config_key] = {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"val_accuracies\": val_accuracies,\n",
    "            \"batch_losses_train\": batch_losses_train_all,\n",
    "            \"batch_losses_valid\": batch_losses_valid_all,\n",
    "            \"model_path\": model_path,\n",
    "            \"best_val_acc\": best_val_acc,\n",
    "            \"params\": {\n",
    "                \"hidden_size\": hidden_size,\n",
    "                \"num_layers\": num_layers\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config_name, data in rnn_results.items():\n",
    "    print(f\"\\n=== RNN Configuration: {config_name} ===\")\n",
    "\n",
    "    # Odtwórz model RNN\n",
    "    model = SpeechCommandRNN(\n",
    "        input_size=N_MELS,\n",
    "        hidden_size=data[\"params\"][\"hidden_size\"],\n",
    "        num_layers=data[\"params\"][\"num_layers\"],\n",
    "        num_classes=len(classes)\n",
    "    ).to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load(data[\"model_path\"]))\n",
    "    model.eval()\n",
    "\n",
    "    # Rysuj wykres lossów\n",
    "    training_details_plot(\n",
    "        batch_loses_train=data[\"batch_losses_train\"],\n",
    "        batch_loses_validate=data[\"batch_losses_valid\"],\n",
    "        title=f\"Loss per iteration - {config_name} (RNN)\",\n",
    "        smoothness=0.98\n",
    "    )\n",
    "\n",
    "    # Predykcja na zbiorze testowym\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Wyświetl metryki i confusion matrix\n",
    "    score_model(all_labels, all_preds, model_name=f\"RNN {config_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn_model = SpeechCommandRNN(\n",
    "#     input_size=N_MELS,\n",
    "#     hidden_size=128,\n",
    "#     num_layers=2,\n",
    "#     num_classes=len(classes)\n",
    "# ).to(device)\n",
    "\n",
    "# optimizer_rnn = torch.optim.Adam(rnn_model.parameters(), lr=1e-4)\n",
    "# criterion_rnn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_val_acc = 0\n",
    "# for epoch in range(1, 11):\n",
    "#     train_loss, train_acc, batch_train = train(rnn_model, train_loader, optimizer_rnn, criterion_rnn, epoch, device)\n",
    "#     val_loss, val_acc, batch_val = evaluate(rnn_model, val_loader, criterion_rnn, device)\n",
    "    \n",
    "#     if val_acc > best_val_acc:\n",
    "#         best_val_acc = val_acc\n",
    "#         print(f\"[LOG] New best model saved: {model_path} (Val Acc: {val_acc:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poniżej Część twoja z CNNem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Input shape: [B, 1, 128, 32]\n",
    "            nn.Conv2d(1, 32, 3, stride=2, padding=1),  # -> [B, 32, 64, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # -> [B, 32, 32, 8]\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),  # -> [B, 64, 16, 4]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # -> [B, 64, 8, 2]\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 2, num_classes)  # Corrected input features\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "# Add mel-spectrogram transformation\n",
    "mel_spectrogram = T.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    n_mels=N_MELS\n",
    ")\n",
    "\n",
    "# Create datasets with different transforms\n",
    "cnn_dataset = SpeechCommandsDataset(\n",
    "    root_dir=\"./data/train/audio/\",\n",
    "    classes=classes,\n",
    "    transform=mel_spectrogram\n",
    ")\n",
    "\n",
    "# Split dataset\n",
    "cnn_train_size = int(0.8 * len(cnn_dataset))\n",
    "cnn_val_size = (len(cnn_dataset) - cnn_train_size) // 2\n",
    "cnn_test_size = len(cnn_dataset) - cnn_train_size - cnn_val_size\n",
    "cnn_train_dataset, cnn_val_dataset, cnn_test_dataset = random_split(cnn_dataset, [cnn_train_size, cnn_val_size, cnn_test_size])\n",
    "\n",
    "# CNN DataLoader with proper collate_fn\n",
    "def cnn_collate_fn(batch):\n",
    "    spectrograms = torch.stack([item[0] for item in batch])  # [B, 1, N_MELS, TIME]\n",
    "    labels = torch.tensor([item[1] for item in batch])\n",
    "    return spectrograms.to(device), labels.to(device)\n",
    "\n",
    "# Update DataLoaders\n",
    "cnn_train_loader = DataLoader(cnn_train_dataset, batch_size=64, \n",
    "                            shuffle=True, collate_fn=cnn_collate_fn)\n",
    "cnn_val_loader = DataLoader(cnn_val_dataset, batch_size=64, \n",
    "                           collate_fn=cnn_collate_fn)\n",
    "cnn_test_loader = DataLoader(cnn_test_dataset, batch_size=64,\n",
    "                            collate_fn=cnn_collate_fn)\n",
    "\n",
    "# Initialize CNN Model\n",
    "cnn_model = SimpleCNN(num_classes=len(classes)).to(device)\n",
    "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Modified training/evaluation functions for CNN\n",
    "def train_cnn(model, loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pred = outputs.argmax(dim=1)\n",
    "        correct += pred.eq(target).sum().item()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"Epoch {epoch} | Batch {batch_idx} | Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    acc = 100. * correct / len(loader.dataset)\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"CNN Train Epoch: {epoch}\\tLoss: {avg_loss:.4f}\\tAccuracy: {acc:.2f}%\")\n",
    "    return acc\n",
    "\n",
    "def evaluate_cnn(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, target)\n",
    "            total_loss += loss.item()\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    \n",
    "    acc = 100. * correct / len(loader.dataset)\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    print(f\"CNN Validation: Loss: {avg_loss:.4f}\\tAccuracy: {acc:.2f}%\")\n",
    "    return acc\n",
    "\n",
    "# Training loop for CNN\n",
    "print(\"Training CNN Model:\")\n",
    "best_cnn_acc = 0\n",
    "for epoch in range(1, 11):\n",
    "    train_cnn(cnn_model, cnn_train_loader, cnn_optimizer, criterion, epoch)\n",
    "    val_acc = evaluate_cnn(cnn_model, cnn_val_loader, criterion)\n",
    "    \n",
    "    if val_acc > best_cnn_acc:\n",
    "        best_cnn_acc = val_acc\n",
    "        torch.save(cnn_model.state_dict(), \"best_cnn_model.pth\")\n",
    "\n",
    "# Evaluate CNN on test set\n",
    "cnn_model.load_state_dict(torch.load(\"best_cnn_model.pth\"))\n",
    "cnn_model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in cnn_test_loader:\n",
    "        outputs = cnn_model(data)\n",
    "        pred = outputs.argmax(dim=1)\n",
    "        y_true.extend(target.cpu().numpy())\n",
    "        y_pred.extend(pred.cpu().numpy())\n",
    "\n",
    "# CNN Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "disp.plot(ax=ax, xticks_rotation='vertical')\n",
    "plt.title(\"CNN Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"CNN Test Accuracy: {100 * np.mean(np.array(y_pred) == np.array(y_true)):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To jakaś próba do usunięcia co testowane było łatwiejsze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "\n",
    "# Simplified configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Tiny test dataset (2 classes, 10 samples each)\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, root=\"data/train/audio\"):\n",
    "        self.classes = ['yes', 'no']  # Only 2 classes\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "        self.samples = []\n",
    "        \n",
    "        # Validate directory structure\n",
    "        if not os.path.exists(root):\n",
    "            raise FileNotFoundError(f\"Directory {root} not found!\")\n",
    "            \n",
    "        for cls in self.classes:\n",
    "            cls_dir = os.path.join(root, cls)\n",
    "            if not os.path.isdir(cls_dir):\n",
    "                raise FileNotFoundError(f\"Class directory {cls_dir} missing!\")\n",
    "                \n",
    "            files = [f for f in os.listdir(cls_dir)[:10]]  # First 10 files\n",
    "            class_idx = self.class_to_idx[cls]\n",
    "            self.samples += [(os.path.join(cls_dir, f), class_idx) for f in files]\n",
    "\n",
    "        # Validate labels\n",
    "        labels = [s[1] for s in self.samples]\n",
    "        assert len(labels) > 0, \"No samples found!\"\n",
    "        assert max(labels) < len(self.classes), f\"Invalid label {max(labels)} for {len(self.classes)} classes\"\n",
    "        assert min(labels) >= 0, \"Negative labels found!\"\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(self.samples[idx][0])\n",
    "            \n",
    "            # Ensure 1 channel\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = waveform[:1, :]\n",
    "                \n",
    "            # Fix length to exactly 16000 samples\n",
    "            if waveform.shape[1] < 16000:\n",
    "                # Pad with zeros\n",
    "                padding = (0, 16000 - waveform.shape[1])\n",
    "                waveform = torch.nn.functional.pad(waveform, padding)\n",
    "            else:\n",
    "                # Trim to 16000\n",
    "                waveform = waveform[:, :16000]\n",
    "                \n",
    "            return waveform, self.samples[idx][1]\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {self.samples[idx][0]}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Tiny model\n",
    "class TestModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(1, 4, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4, 2)  # 2 classes\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Quick test pipeline\n",
    "model = TestModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Modify the DataLoader with collate_fn\n",
    "# Modified collate function\n",
    "def collate_fn(batch):\n",
    "    waveforms = [item[0] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "    \n",
    "    # Stack to [batch_size, 1, 16000]\n",
    "    waveforms = torch.stack(waveforms)\n",
    "    labels = torch.tensor(labels)\n",
    "    return waveforms.to(device), labels.to(device)\n",
    "\n",
    "# Update DataLoader\n",
    "train_loader = DataLoader(\n",
    "    TestDataset(),\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "\n",
    "# Test training loop (2 epochs)\n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    for batch, (x, y) in enumerate(train_loader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 2 == 0:\n",
    "            print(f\"Epoch {epoch+1} | Batch {batch} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Quick validation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_x = torch.randn(4, 1, 16000).to(device)  # Batch of 4\n",
    "    preds = model(test_x).argmax(dim=1)\n",
    "    print(\"\\nSample predictions:\", preds.cpu().numpy())\n",
    "    print(\"If you see changing losses and random predictions, it works!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
